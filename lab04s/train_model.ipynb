{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conda create -n pyspark python=3.6.8 pip wheel pandas matplotlib ipykernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.3.2.3.1.0.0-78\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.8 (default, Dec 30 2018 01:22:34)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='pyspark-shell --master yarn ' # local[0]'\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/usr/bin/python3'\n",
    "#PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"]='/usr/bin/python3'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_schema = StructType(\n",
    "   fields = [\n",
    "      StructField(\"uid\", StringType(), True),\n",
    "      StructField(\"gender_age\", StringType(), True),\n",
    "      StructField(\"visits\", ArrayType(\n",
    "          StructType([\n",
    "               StructField(\"timestamp\", LongType(), True),\n",
    "               StructField(\"url\", StringType(), True)\n",
    "               ])\n",
    "      ),True)\n",
    "   ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_schema = StructType(\n",
    "   fields = [\n",
    "      StructField(\"uid\", StringType(), True),\n",
    "      StructField(\"visits\", ArrayType(\n",
    "          StructType([\n",
    "               StructField(\"timestamp\", LongType(), True),\n",
    "               StructField(\"url\", StringType(), True)\n",
    "               ])\n",
    "      ),True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/41399399/serialize-a-custom-transformer-using-python-to-be-used-within-a-pyspark-ml-pipel\n",
    "\n",
    "https://www.slideshare.net/SparkSummit/building-custom-ml-pipelinestages-for-feature-selection-with-marc-kaminski\n",
    "\n",
    "https://stackoverflow.com/questions/42140980/spark-ml-pipelines-unseen-label-exception-when-classifying-new-examples?rq=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param, Params\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "\n",
    "class Url2DomainTransformer(Transformer, DefaultParamsReadable, DefaultParamsWritable):\n",
    "\n",
    "    def __init__(self, inputCol=\"visits.url\", outputCol=\"urls\"):\n",
    "        super(Url2DomainTransformer, self).__init__()\n",
    "        self.inputCol = inputCol\n",
    "        self.outputCol = outputCol\n",
    "    \n",
    "     \n",
    "    def _transform(self, dataset):\n",
    "        import re\n",
    "        from urllib.parse import urlparse\n",
    "        from urllib.request import urlretrieve, unquote\n",
    "        from pyspark.sql import functions as F \n",
    "        \n",
    "        def url2domain(url):\n",
    "            url = re.sub('(http(s)*://)+', 'http://', url)\n",
    "            parsed_url = urlparse(unquote(url.strip()))\n",
    "            if parsed_url.scheme not in ['http','https']: return None\n",
    "            netloc = re.search(\"(?:www\\.)?(.*)\", parsed_url.netloc).group(1)\n",
    "            if netloc is not None: return str(netloc.encode('utf8')).strip()\n",
    "            return None    \n",
    "        \n",
    "        url2domain_udf = F.udf(lambda xx: [ url2domain(x) for x in xx],\n",
    "                   ArrayType(StringType()))\n",
    "        dataset = dataset.withColumn(self.outputCol,url2domain_udf(self.inputCol))\n",
    "        return dataset\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelectFields(Transformer, DefaultParamsReadable, DefaultParamsWritable):\n",
    "\n",
    "    #@keyword_only\n",
    "    def __init__(self, selectFields=[\"uid\",\"gender_age\",\"urls\"]):\n",
    "        super(SelectFields, self).__init__()\n",
    "        self.selectFields = selectFields\n",
    "        \n",
    "    def _transform(self, dataset):\n",
    "        return dataset.select(self.selectFields)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_strings = ['M:25-34',\n",
    " 'F:25-34',\n",
    " 'M:35-44',\n",
    " 'F:35-44',\n",
    " 'F:18-24',\n",
    " 'F:45-54',\n",
    " 'M:45-54',\n",
    " 'M:18-24',\n",
    " 'F:>=55',\n",
    " 'M:>=55']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = \"lab04/lab04_train_merged_labels.json\"\n",
    "df_train = spark.read.json(training, train_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                 url|\n",
      "+--------------------+\n",
      "|[http://zebra-zoy...|\n",
      "|[http://sweetradi...|\n",
      "|[http://ru.orifla...|\n",
      "|[http://translate...|\n",
      "|[https://mail.ram...|\n",
      "|[https://cfire.ma...|\n",
      "|[http://www.msn.c...|\n",
      "|[http://www.gazpr...|\n",
      "|[http://lifenews....|\n",
      "|[https://www.goog...|\n",
      "|[http://muz4in.ne...|\n",
      "|[http://kosmetist...|\n",
      "|[http://android.m...|\n",
      "|[http://tsn.ua/po...|\n",
      "|[http://www.jobin...|\n",
      "|[http://www.abc-p...|\n",
      "|[http://easygames...|\n",
      "|[http://www.ratan...|\n",
      "|[http://sam-zdrav...|\n",
      "|[http://www.msn.c...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.select(\"visits.url\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasOutputCols, Param, Params\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "from pyspark.sql.functions import lit # for the dummy _transform\n",
    "\n",
    "class SetValueTransformer(\n",
    "    Transformer, HasOutputCols, DefaultParamsReadable, DefaultParamsWritable,\n",
    "):\n",
    "    value = Param(\n",
    "        Params._dummy(),\n",
    "        \"value\",\n",
    "        \"value to fill\",\n",
    "    )\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self, outputCols=None, value=0.0):\n",
    "        super(SetValueTransformer, self).__init__()\n",
    "        self._setDefault(value=0.0)\n",
    "        kwargs = self._input_kwargs\n",
    "        self._set(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, outputCols=None, value=0.0):\n",
    "        \"\"\"\n",
    "        setParams(self, outputCols=None, value=0.0)\n",
    "        Sets params for this SetValueTransformer.\n",
    "        \"\"\"\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def setValue(self, value):\n",
    "        \"\"\"\n",
    "        Sets the value of :py:attr:`value`.\n",
    "        \"\"\"\n",
    "        return self._set(value=value)\n",
    "\n",
    "    def getValue(self):\n",
    "        \"\"\"\n",
    "        Gets the value of :py:attr:`value` or its default value.\n",
    "        \"\"\"\n",
    "        return self.getOrDefault(self.value)\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        \n",
    "        dataset = dataset.withColumn(self.getOutputCols()[0], lit(self.getValue()))\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#indexer.fit(df_train.limit(10)).labels\n",
    "df_train_cut = df_train.limit(10)\n",
    "#labels = indexer.fit(df_train_cut).labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import StringIndexer, IndexToString\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "url2domain_transformer = Url2DomainTransformer(outputCol=\"urls\",inputCol=\"visits.url\")\n",
    "select_transformer = SelectFields(selectFields=[\"uid\",\"gender_age\",\"urls\"])\n",
    "indexer = StringIndexer(inputCol=\"gender_age\", outputCol=\"label\", handleInvalid=\"keep\")\n",
    "pipeline_transform = Pipeline(stages =  [url2domain_transformer, select_transformer, indexer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"urls\", outputCol=\"features\")\n",
    "\n",
    "#labels = indexer.fit(df_train_cut).labels\n",
    "#df_train_cut=indexer.fit(df_train_cut).transform(df_train_cut)\n",
    "\n",
    "#transformer = SetValueTransformer(outputCols=[\"a\"])\n",
    "\n",
    "lr = LogisticRegression(labelCol='label', probabilityCol='lr_probability', predictionCol='lr_prediction', rawPredictionCol='lr_rawPrediction')\n",
    "\n",
    "#rf = RandomForestClassifier(labelCol='label', probabilityCol='rf_probability', predictionCol='rf_prediction', rawPredictionCol='rf_rawPrediction')\n",
    "\n",
    "#lr_label_converter = IndexToString(inputCol=\"lr_prediction\", outputCol=\"lr_gender_age\", labels=label_strings)\n",
    "\n",
    "#rf_label_converter = IndexToString(inputCol=\"rf_prediction\", outputCol=\"rf_gender_age\", labels=label_strings)\n",
    "lr_label_converter = IndexToString(inputCol=\"lr_prediction\", outputCol=\"lr_gender_age\", labels=label_strings)\n",
    "\n",
    "pipeline = Pipeline(stages =  [url2domain_transformer,select_transformer, indexer, cv, lr, lr_label_converter])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trasform = pipeline_transform.fit(df_train_cut)\n",
    "model_trasform.write().overwrite().save(\"tst_custom_transformer_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "model_transform_reloaded =  PipelineModel.load(\"tst_custom_transformer_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_cut_transformed = model_transform_reloaded.transform(df_train_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+-----+\n",
      "|                 uid|gender_age|                urls|label|\n",
      "+--------------------+----------+--------------------+-----+\n",
      "|d50192e5-c44e-4ae...|   F:18-24|[b'zebra-zoya.ru'...|  1.0|\n",
      "|d502331d-621e-472...|   M:25-34|[b'sweetrading.ru...|  4.0|\n",
      "|d50237ea-747e-48a...|   F:25-34|[b'ru.oriflame.co...|  0.0|\n",
      "|d502f29f-d57a-46b...|   F:25-34|[b'translate-tatt...|  0.0|\n",
      "|d503c3b2-a0c2-4f4...|    M:>=55|[b'mail.rambler.r...|  2.0|\n",
      "|d5090ddf-5648-487...|   F:25-34|[b'cfire.mail.ru'...|  0.0|\n",
      "|d50bcef8-16ff-4e8...|   F:25-34|[b'msn.com', b'ms...|  0.0|\n",
      "|d50e23dc-0cbd-488...|   F:18-24|[b'gazprom.ru', b...|  1.0|\n",
      "|d50fdabb-4208-441...|   F:45-54|[b'lifenews.ru', ...|  3.0|\n",
      "|d511b480-23a6-482...|   F:18-24|[b'google.ru', b'...|  1.0|\n",
      "+--------------------+----------+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train_cut_transformed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(df_train_cut_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "lr_evaluator = BinaryClassificationEvaluator(\n",
    "        rawPredictionCol='lr_rawPrediction', labelCol='label', metricName='areaUnderROC'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "#regParam for L2 regularization, https://craftappmobile.com/l1-vs-l2-regularization/\n",
    "#elasticNetParam https://en.wikipedia.org/wiki/Elastic_net_regularization, https://www.quora.com/What-is-elastic-net-regularization-in-machine-learning\n",
    "\n",
    "grid = ParamGridBuilder() \\\n",
    "    .addGrid(LogisticRegression.regParam, [0.1, 0.01]) \\\n",
    "    .addGrid(LogisticRegression.elasticNetParam, [0.1, 0.01]) \\\n",
    "    .build()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=grid,\n",
    "    evaluator=lr_evaluator,\n",
    "    numFolds=3,\n",
    "    parallelism=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+\n",
      "|                 uid|gender_age|              visits|\n",
      "+--------------------+----------+--------------------+\n",
      "|d50192e5-c44e-4ae...|   F:18-24|[[1419688144068, ...|\n",
      "|d502331d-621e-472...|   M:25-34|[[1419717886224, ...|\n",
      "|d50237ea-747e-48a...|   F:25-34|[[1418840296062, ...|\n",
      "|d502f29f-d57a-46b...|   F:25-34|[[1418217864467, ...|\n",
      "|d503c3b2-a0c2-4f4...|    M:>=55|[[1427272415001, ...|\n",
      "|d5090ddf-5648-487...|   F:25-34|[[1419777541435, ...|\n",
      "|d50bcef8-16ff-4e8...|   F:25-34|[[1426704753001, ...|\n",
      "|d50e23dc-0cbd-488...|   F:18-24|[[1419613709992, ...|\n",
      "|d50fdabb-4208-441...|   F:45-54|[[1427203859001, ...|\n",
      "|d511b480-23a6-482...|   F:18-24|[[1427237735001, ...|\n",
      "+--------------------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train_cut.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel = crossval.fit(df_train.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "handleInvalid: how to handle invalid data (unseen or NULL values) in features and label column of string type. Options are 'skip' (filter out rows with invalid data), error (throw an error), or 'keep' (put invalid data in a special additional bucket, at index numLabels). (default: error)\n",
      "inputCol: input column name. (current: gender_age)\n",
      "outputCol: output column name. (default: StringIndexer_41b393718a28418251d5__output, current: label)\n",
      "stringOrderType: How to order labels of string column. The first label after ordering is assigned an index of 0. Supported options: frequencyDesc, frequencyAsc, alphabetDesc, alphabetAsc. (default: frequencyDesc)\n"
     ]
    }
   ],
   "source": [
    "print(indexer.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel.bestModel.write().overwrite().save(\"tst_cv_bestmodel_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestmodel_reloaded =  PipelineModel.load(\"tst_cv_bestmodel_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+-----+--------------------+--------------------+--------------------+-------------+-------------+\n",
      "|                 uid|gender_age|                urls|label|            features|    lr_rawPrediction|      lr_probability|lr_prediction|lr_gender_age|\n",
      "+--------------------+----------+--------------------+-----+--------------------+--------------------+--------------------+-------------+-------------+\n",
      "|dd387df7-f50e-4b7...|   F:25-34|[b'zdorovie43.gor...|  1.0|(128,[0,3,4,9,12,...|[3.08571958527996...|[3.47993295511760...|          5.0|      F:45-54|\n",
      "|dd3aa566-00da-491...|   M:45-54|[b'mail.rambler.r...|  3.0|(128,[0,5,7,26,55...|[4.27123181104901...|[9.77632606185703...|          5.0|      F:45-54|\n",
      "|dd3adce0-6207-479...|   F:25-34|[b'pokupkalux.ru'...|  1.0|(128,[1,2,6,11,13...|[-1.8633393060618...|[6.46458303077854...|          1.0|      F:25-34|\n",
      "|dd3bebdf-69ea-4fd...|   M:35-44|[b'biznes-doman.c...|  4.0|    (128,[81],[1.0])|[5.30497666103927...|[0.12499994046580...|          1.0|      F:25-34|\n",
      "|dd41e32e-202a-424...|   M:25-34|[b'wf.mail.ru', b...|  0.0|    (128,[8],[15.0])|[5.30497666103927...|[0.12499994046580...|          1.0|      F:25-34|\n",
      "|dd45f6b7-675c-414...|   F:18-24|[b'yves-rocher.ru...|  2.0|(128,[2,10,13,14,...|[-0.4296761126415...|[2.25428079846066...|          1.0|      F:25-34|\n",
      "|dd472fa9-d1ee-4a5...|   M:25-34|[b'muzofon.com', ...|  0.0|(128,[13,53,76],[...|[3.87131346761905...|[9.55644429956847...|          1.0|      F:25-34|\n",
      "|dd474635-14fd-483...|   F:25-34|[b'share.pho.to',...|  1.0|(128,[84,101],[1....|[5.30497666103927...|[0.12499994046580...|          1.0|      F:25-34|\n",
      "|dd489bce-c115-463...|   M:25-34|[b'myhome.ru', b'...|  0.0|(128,[10,13,70],[...|[3.87131346761905...|[9.55644429956847...|          1.0|      F:25-34|\n",
      "|dd4b6f79-3d65-4f5...|   M:25-34|[b'oilcareer.ru',...|  0.0|(128,[17,18,30,35...|[5.30497666103927...|[0.12499994046580...|          1.0|      F:25-34|\n",
      "+--------------------+----------+--------------------+-----+--------------------+--------------------+--------------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bestmodel_reloaded.transform(df_train.limit(10)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"handleInvalid: how to handle invalid data (unseen or NULL values) in features and label column of string type. Options are 'skip' (filter out rows with invalid data), error (throw an error), or 'keep' (put invalid data in a special additional bucket, at index numLabels). (default: error)\\ninputCol: input column name. (undefined)\\noutputCol: output column name. (default: StringIndexer_478e9078f8887914014c__output)\\nstringOrderType: How to order labels of string column. The first label after ordering is assigned an index of 0. Supported options: frequencyDesc, frequencyAsc, alphabetDesc, alphabetAsc. (default: frequencyDesc)\""
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StringIndexer().explainParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_cut = df_train.limit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+-----+\n",
      "|                 uid|gender_age|              visits|label|\n",
      "+--------------------+----------+--------------------+-----+\n",
      "|dd387df7-f50e-4b7...|   F:25-34|[[1419606827095, ...|  1.0|\n",
      "|dd3aa566-00da-491...|   M:45-54|[[1426958952000, ...|  3.0|\n",
      "|dd3adce0-6207-479...|   F:25-34|[[1414428818001, ...|  1.0|\n",
      "|dd3bebdf-69ea-4fd...|   M:35-44|[[1422207817000, ...|  4.0|\n",
      "|dd41e32e-202a-424...|   M:25-34|[[1418813830629, ...|  0.0|\n",
      "|dd45f6b7-675c-414...|   F:18-24|[[1419921965314, ...|  2.0|\n",
      "|dd472fa9-d1ee-4a5...|   M:25-34|[[1418674371440, ...|  0.0|\n",
      "|dd474635-14fd-483...|   F:25-34|[[1418405925940, ...|  1.0|\n",
      "|dd489bce-c115-463...|   M:25-34|[[1427219718001, ...|  0.0|\n",
      "|dd4b6f79-3d65-4f5...|   M:25-34|[[1427186182000, ...|  0.0|\n",
      "+--------------------+----------+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train_cut.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['uid', 'gender_age', 'urls']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_transformer.selectFields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[uid: string, gender_age: string, visits: array<struct<timestamp:bigint,url:string>>, label: double]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[uid: string, gender_age: string, visits: array<struct<timestamp:bigint,url:string>>, label: double]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel.bestModel.write().overwrite().save('lab04s_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stages: a list of pipeline stages (current: [Url2DomainTransformer_447b82347803217117ef, SelectFields_47998934796a145d7cf0, CountVectorizer_461ea09a378ed3faa2e4, StringIndexer_4b4aad7b8abe1f399a9c, LogisticRegression_44ce84cc5481de82497c, IndexToString_41cfb7bd197702f8c352])'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.explainParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot resolve 712 as a param.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-4974638cbef8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplainParam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m712\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/ml/param/__init__.py\u001b[0m in \u001b[0;36mexplainParam\u001b[0;34m(self, param)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0mdefault\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msupplied\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \"\"\"\n\u001b[0;32m--> 278\u001b[0;31m         \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resolveParam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misDefined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/ml/param/__init__.py\u001b[0m in \u001b[0;36m_resolveParam\u001b[0;34m(self, param)\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetParam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot resolve %r as a param.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot resolve 712 as a param."
     ]
    }
   ],
   "source": [
    "pipeline.explainParam(712)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel = crossval.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Url2DomainTransformer' object has no attribute '_to_java'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-090c80967614>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcvModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lab04s_model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/ml/util.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;34m\"\"\"Save this ML instance to the given path, a shortcut of 'write().save(path)'.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/ml/tuning.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    411\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m         \u001b[0;34m\"\"\"Returns an MLWriter instance for this ML instance.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mJavaMLWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/ml/util.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, instance)\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJavaMLWriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0m_java_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/ml/tuning.py\u001b[0m in \u001b[0;36m_to_java\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    446\u001b[0m         _java_obj = JavaParams._new_java_obj(\"org.apache.spark.ml.tuning.CrossValidatorModel\",\n\u001b[1;32m    447\u001b[0m                                              \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m                                              \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbestModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m                                              _py2java(sc, []))\n\u001b[1;32m    450\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCrossValidatorModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_to_java\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0mjava_stages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m             \u001b[0mjava_stages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0m_java_obj\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Url2DomainTransformer' object has no attribute '_to_java'"
     ]
    }
   ],
   "source": [
    "cvModel.write.('lab04s_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверю модель на урлах, открытых в моём браузере"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "  \"uid\": \"bd7a30e1-a25d-4cbf-a03f-61748cbe540e\",\n",
    "  \"visits\": [\n",
    "    {\n",
    "      \"url\": \"https://mail.google.com/mail/u/0/#inbox\",\n",
    "      \"timestamp\": 1419775945781\n",
    "    }\n",
    "   ,  \n",
    "   {\n",
    "      \"url\": \"https://lk-de.newprolab.com/\",\n",
    "      \"timestamp\": 1419775945781\n",
    "    }\n",
    "   ,  \n",
    "   {\n",
    "      \"url\": \"https://yandex.ru/pogoda/moscow/maps/temperature?via=mmapwb&le_TemperatureBalloons=0&le_WindParticles=1&ll=25.976425_49.047348&z=4\",\n",
    "      \"timestamp\": 1419775945781\n",
    "    }\n",
    "  ,  \n",
    "   {\n",
    "      \"url\": \"https://translate.yandex.ru/?lang=en-ru&text=derivation\",\n",
    "      \"timestamp\": 1419775945781\n",
    "    }\n",
    " ,  \n",
    "   {\n",
    "      \"url\": \"https://web.whatsapp.com/\",\n",
    "      \"timestamp\": 1419775945781\n",
    "    }\n",
    ",  \n",
    "   {\n",
    "      \"url\": \"https://app.slack.com/client/TNG296ABE/CPPRL95HU/thread/CP73F91ST-1571040655.075700\",\n",
    "      \"timestamp\": 1419775945781\n",
    "    }\n",
    ",  \n",
    "   {\n",
    "      \"url\": \"https://github.com/newprolab/content_dataengineer5/blob/master/labs/de_lab_04.md\",\n",
    "      \"timestamp\": 1419775945781\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = spark.read.json(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                 uid|              visits|\n",
      "+--------------------+--------------------+\n",
      "|bd7a30e1-a25d-4cb...|[[1419775945781, ...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.withColumn(\"urls\",url2domain_udf(df_test[\"visits\"].getField(\"url\"))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.select([\"uid\", \"urls\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+----------+\n",
      "|uid                                 |urls                                                                                                                                    |features                              |rawPrediction                                                                                                                                                                                    |probability                                                                                                                                                                                                         |prediction|gender_age|\n",
      "+------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+----------+\n",
      "|bd7a30e1-a25d-4cbf-a03f-61748cbe540e|[b'mail.google.com', b'lk-de.newprolab.com', b'yandex.ru', b'translate.yandex.ru', b'web.whatsapp.com', b'app.slack.com', b'github.com']|(111581,[7,20934,60121],[1.0,1.0,1.0])|[-2.1159833078154517,5.385125914530741,-1.4936937081482853,9.463097073490633,-0.4739966272089833,-5.766289427852555,-1.3248684041341,-0.9597944320748362,-1.2413960824400332,-1.4722009983471311]|[9.202424876708688E-6,0.016656840389225873,1.7145857899538582E-5,0.983179910433438,4.753440209314583E-5,2.391082030308779E-7,2.0299209137850006E-5,2.9243458236243097E-5,2.206635936799694E-5,1.7518357521648507E-5]|3.0       |F:35-44   |\n",
      "+------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_reloaded.transform(df_test).show(1,False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyspark]",
   "language": "python",
   "name": "conda-env-pyspark-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
